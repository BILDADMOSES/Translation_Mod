{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "403f5b87",
   "metadata": {},
   "source": [
    "## Brief description of the data set and a summary of its attributes\n",
    "My data set is made up of two columns: English words/sentences and French words/sentences.\n",
    "\n",
    "It contains 175622 rows, of which there wasn’t any null values.\n",
    "\n",
    "I pulled the data set from Kaggle as I needed data containing proper and correct translation of both languages. I am also fluent only in one of the languages, English thus it provided me with a ready made translation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e19cca4",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries\n",
    "- pandas\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498f79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9021e2c",
   "metadata": {},
   "source": [
    "## Using pandas library to retrieve the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aebce4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"C:\\Users\\Bildad Otieno\\Documents\\Billy_Repo\\Translation_Mod\\Eng-Fre.csv\"\n",
    "df = pd.read_csv(file, encoding= 'utf-8')\n",
    "df = df.replace('�','',regex = True)\n",
    "#df.to_csv(\"C:\\\\Users\\\\Bildad Otieno\\\\Documents\\\\Billy_Repo\\\\Translation_Mod\\\\Eng-Fre2.csv\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9332c7f",
   "metadata": {},
   "source": [
    "## Printing out the first 5 rows of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42883f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez?!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>?a alors?!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours?!\n",
       "2                    Run!               Courez?!\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             ?a alors?!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "529d4684",
   "metadata": {},
   "source": [
    "## Checking for any null values\n",
    "Checking for missing values: \n",
    "- **df.isnull()** or **df.isna()** - will return true if null\n",
    "- **df.notnull()** - will return true false if null\n",
    "\n",
    "Handling missing values:\n",
    "1)   Removing rows or columns with missing values: **df.dropna()**\n",
    "2)   Interpolating missing values: **df.interpolate()**\n",
    "3)   Imputing missing values: You can use **df.fillna(value)** to fill missing values with a specific value, or use more advanced techniques like mean, median, or machine learning algorithms for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b932e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English words/sentences    0\n",
       "French words/sentences     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a20e2116",
   "metadata": {},
   "source": [
    "## Checking for unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b57fbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289032"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faf9a543",
   "metadata": {},
   "source": [
    "## Checking the number of rows\n",
    "Shape function will return a tuple consisting of 2 indices, 1st (rows,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54508b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175621"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e6b6987",
   "metadata": {},
   "source": [
    "## Checking for number of records\n",
    "We also could use this to see the number of records in every column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1808bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English words/sentences    175621\n",
       "French words/sentences     175621\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb2eb088",
   "metadata": {},
   "source": [
    "## Checking for the data types of values within the dataframe\n",
    "We could use **astype(dtype)** to change the data type of records e.g. df.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f17c09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English words/sentences    object\n",
       "French words/sentences     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cae2dcc",
   "metadata": {},
   "source": [
    "## Checking for number of duplicates\n",
    "- Detecting duplicates: **df.duplicated()** to check for duplicate rows.\n",
    "- Removing duplicates: **df.drop_duplicates()** to remove duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e68e72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1dd6993",
   "metadata": {},
   "source": [
    "## Printing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d91cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>Stand back!</td>\n",
       "      <td>Reculez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8626</th>\n",
       "      <td>It's not funny.</td>\n",
       "      <td>Ce n'est pas dr?le !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65263</th>\n",
       "      <td>What time will you leave?</td>\n",
       "      <td>? quelle heure pars-tu??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100309</th>\n",
       "      <td>What's the weather like today?</td>\n",
       "      <td>Quel temps fait-il aujourd'hui??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147869</th>\n",
       "      <td>Medical marijuana is legal in this state.</td>\n",
       "      <td>La marijuana th?rapeutique est l?gale dans cet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          English words/sentences  \\\n",
       "1621                                  Stand back!   \n",
       "8626                              It's not funny.   \n",
       "65263                   What time will you leave?   \n",
       "100309             What's the weather like today?   \n",
       "147869  Medical marijuana is legal in this state.   \n",
       "\n",
       "                                   French words/sentences  \n",
       "1621                                            Reculez !  \n",
       "8626                                 Ce n'est pas dr?le !  \n",
       "65263                            ? quelle heure pars-tu??  \n",
       "100309                   Quel temps fait-il aujourd'hui??  \n",
       "147869  La marijuana th?rapeutique est l?gale dans cet...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = df[df.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8741de86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175616</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175617</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175618</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175619</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175620</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175621 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        English words/sentences  French words/sentences\n",
       "0                         False                   False\n",
       "1                         False                   False\n",
       "2                         False                   False\n",
       "3                         False                   False\n",
       "4                         False                   False\n",
       "...                         ...                     ...\n",
       "175616                    False                   False\n",
       "175617                    False                   False\n",
       "175618                    False                   False\n",
       "175619                    False                   False\n",
       "175620                    False                   False\n",
       "\n",
       "[175621 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f71f6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eng, Fre = df[\"English words/sentences\"], df[\"French words/sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c855805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Printing out a collection of punctuation marks, ASCII characters\n",
    "print(string.punctuation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2cb05f5",
   "metadata": {},
   "source": [
    "## Removing the Punctuation Marks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31237e78",
   "metadata": {},
   "source": [
    "Initially I did this but then realized that I wasn't really using the fully capabilities of the <span style = \"color:red\">if statement</span>. You notice that I am instead using the else statement to append the letters to my **col** list.\n",
    "\n",
    "    def remove_punc(column):\n",
    "        new_column = []\n",
    "        for word in column:\n",
    "            col = [] \n",
    "            for letter in word:\n",
    "                if letter in string.punctuation:\n",
    "                    letter = letter.replace(letter,'')\n",
    "                else:\n",
    "                    col.append(letter) #list for individual letters now without punctuation mark\n",
    "                new_word = \"\".join(col)\n",
    "            new_column.append(new_word)    \n",
    "        return new_column\n",
    "\n",
    "Instead I used <span style = \"color:blue\">not in</span> which was more effective and cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7558d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(column):\n",
    "    new_column = []\n",
    "    for word in column:\n",
    "        col = []\n",
    "        for letter in word:\n",
    "            if letter not in string.punctuation:\n",
    "                col.append(letter) #list for individual letters now without punctuation mark\n",
    "            new_word = \"\".join(col)\n",
    "        new_column.append(new_word)    \n",
    "    return new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54dde395",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Punc_Eng = remove_punc(Eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cf3087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Punc_Fre = remove_punc(Fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f110edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Bildad\n",
      "[nltk_data]     Otieno/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3670efc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175621"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_Eng = [nltk.word_tokenize(word) for word in No_Punc_Eng]\n",
    "len(tokenized_Eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aedfde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi'],\n",
       " ['Run'],\n",
       " ['Run'],\n",
       " ['Who'],\n",
       " ['Wow'],\n",
       " ['Fire'],\n",
       " ['Help'],\n",
       " ['Jump'],\n",
       " ['Stop'],\n",
       " ['Stop'],\n",
       " ['Stop'],\n",
       " ['Wait'],\n",
       " ['Wait'],\n",
       " ['Go', 'on'],\n",
       " ['Go', 'on'],\n",
       " ['Go', 'on'],\n",
       " ['Hello'],\n",
       " ['Hello'],\n",
       " ['I', 'see'],\n",
       " ['I', 'try'],\n",
       " ['I', 'won'],\n",
       " ['I', 'won'],\n",
       " ['I', 'won'],\n",
       " ['Oh', 'no'],\n",
       " ['Attack'],\n",
       " ['Attack'],\n",
       " ['Cheers'],\n",
       " ['Cheers'],\n",
       " ['Cheers'],\n",
       " ['Cheers'],\n",
       " ['Get', 'up'],\n",
       " ['Go', 'now'],\n",
       " ['Go', 'now'],\n",
       " ['Go', 'now'],\n",
       " ['Got', 'it'],\n",
       " ['Got', 'it'],\n",
       " ['Got', 'it'],\n",
       " ['Got', 'it'],\n",
       " ['Got', 'it'],\n",
       " ['Hop', 'in'],\n",
       " ['Hop', 'in'],\n",
       " ['Hug', 'me'],\n",
       " ['Hug', 'me'],\n",
       " ['I', 'fell'],\n",
       " ['I', 'fell'],\n",
       " ['I', 'know'],\n",
       " ['I', 'left'],\n",
       " ['I', 'left'],\n",
       " ['I', 'lied'],\n",
       " ['I', 'lost'],\n",
       " ['I', 'paid'],\n",
       " ['Im', '19'],\n",
       " ['Im', 'OK'],\n",
       " ['Im', 'OK'],\n",
       " ['Listen'],\n",
       " ['No', 'way'],\n",
       " ['No', 'way'],\n",
       " ['No', 'way'],\n",
       " ['No', 'way'],\n",
       " ['No', 'way'],\n",
       " ['No', 'way'],\n",
       " ['No', 'way'],\n",
       " ['No', 'way'],\n",
       " ['No', 'way'],\n",
       " ['Really'],\n",
       " ['Really'],\n",
       " ['Really'],\n",
       " ['Thanks'],\n",
       " ['We', 'try'],\n",
       " ['We', 'won'],\n",
       " ['We', 'won'],\n",
       " ['We', 'won'],\n",
       " ['We', 'won'],\n",
       " ['Ask', 'Tom'],\n",
       " ['Awesome'],\n",
       " ['Be', 'calm'],\n",
       " ['Be', 'calm'],\n",
       " ['Be', 'calm'],\n",
       " ['Be', 'cool'],\n",
       " ['Be', 'fair'],\n",
       " ['Be', 'fair'],\n",
       " ['Be', 'fair'],\n",
       " ['Be', 'fair'],\n",
       " ['Be', 'fair'],\n",
       " ['Be', 'fair'],\n",
       " ['Be', 'kind'],\n",
       " ['Be', 'nice'],\n",
       " ['Be', 'nice'],\n",
       " ['Be', 'nice'],\n",
       " ['Be', 'nice'],\n",
       " ['Be', 'nice'],\n",
       " ['Be', 'nice'],\n",
       " ['Beat', 'it'],\n",
       " ['Call', 'me'],\n",
       " ['Call', 'me'],\n",
       " ['Call', 'us'],\n",
       " ['Call', 'us'],\n",
       " ['Come', 'in'],\n",
       " ['Come', 'in'],\n",
       " ['Come', 'in'],\n",
       " ['Come', 'in'],\n",
       " ['Come', 'on'],\n",
       " ['Come', 'on'],\n",
       " ['Come', 'on'],\n",
       " ['Come', 'on'],\n",
       " ['Drop', 'it'],\n",
       " ['Drop', 'it'],\n",
       " ['Drop', 'it'],\n",
       " ['Drop', 'it'],\n",
       " ['Get', 'Tom'],\n",
       " ['Get', 'out'],\n",
       " ['Get', 'out'],\n",
       " ['Get', 'out'],\n",
       " ['Get', 'out'],\n",
       " ['Get', 'out'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'away'],\n",
       " ['Go', 'home'],\n",
       " ['Go', 'home'],\n",
       " ['Go', 'home'],\n",
       " ['Go', 'home'],\n",
       " ['Go', 'slow'],\n",
       " ['Go', 'slow'],\n",
       " ['Goodbye'],\n",
       " ['Goodbye'],\n",
       " ['Hang', 'on'],\n",
       " ['Hang', 'on'],\n",
       " ['Hang', 'on'],\n",
       " ['Hang', 'on'],\n",
       " ['He', 'quit'],\n",
       " ['He', 'quit'],\n",
       " ['He', 'runs'],\n",
       " ['Help', 'me'],\n",
       " ['Help', 'me'],\n",
       " ['Help', 'me'],\n",
       " ['Help', 'us'],\n",
       " ['Help', 'us'],\n",
       " ['Hold', 'it'],\n",
       " ['Hold', 'on'],\n",
       " ['Hug', 'Tom'],\n",
       " ['I', 'agree'],\n",
       " ['I', 'cried'],\n",
       " ['I', 'dozed'],\n",
       " ['I', 'dozed'],\n",
       " ['I', 'drive'],\n",
       " ['I', 'smoke'],\n",
       " ['I', 'snore'],\n",
       " ['I', 'stink'],\n",
       " ['I', 'stood'],\n",
       " ['I', 'stood'],\n",
       " ['I', 'swore'],\n",
       " ['I', 'swore'],\n",
       " ['I', 'tried'],\n",
       " ['I', 'tried'],\n",
       " ['I', 'tried'],\n",
       " ['I', 'waved'],\n",
       " ['Ill', 'go'],\n",
       " ['Im', 'Tom'],\n",
       " ['Im', 'fat'],\n",
       " ['Im', 'fat'],\n",
       " ['Im', 'fit'],\n",
       " ['Im', 'hit'],\n",
       " ['Im', 'hit'],\n",
       " ['Im', 'ill'],\n",
       " ['Im', 'sad'],\n",
       " ['Im', 'shy'],\n",
       " ['Im', 'wet'],\n",
       " ['Im', 'wet'],\n",
       " ['Its', 'me'],\n",
       " ['Join', 'us'],\n",
       " ['Join', 'us'],\n",
       " ['Keep', 'it'],\n",
       " ['Keep', 'it'],\n",
       " ['Kiss', 'me'],\n",
       " ['Kiss', 'me'],\n",
       " ['Me', 'too'],\n",
       " ['Open', 'up'],\n",
       " ['Open', 'up'],\n",
       " ['Perfect'],\n",
       " ['See', 'you'],\n",
       " ['See', 'you'],\n",
       " ['See', 'you'],\n",
       " ['See', 'you'],\n",
       " ['Show', 'me'],\n",
       " ['Show', 'me'],\n",
       " ['Shut', 'up'],\n",
       " ['Shut', 'up'],\n",
       " ['Shut', 'up'],\n",
       " ['Shut', 'up'],\n",
       " ['Shut', 'up'],\n",
       " ['Skip', 'it'],\n",
       " ['So', 'long'],\n",
       " ['Take', 'it'],\n",
       " ['Take', 'it'],\n",
       " ['Take', 'it'],\n",
       " ['Take', 'it'],\n",
       " ['Tell', 'me'],\n",
       " ['Tell', 'me'],\n",
       " ['Tom', 'won'],\n",
       " ['Wake', 'up'],\n",
       " ['Wake', 'up'],\n",
       " ['Wake', 'up'],\n",
       " ['Wake', 'up'],\n",
       " ['Wake', 'up'],\n",
       " ['Wash', 'up'],\n",
       " ['Wash', 'up'],\n",
       " ['We', 'know'],\n",
       " ['We', 'lost'],\n",
       " ['We', 'lost'],\n",
       " ['We', 'lost'],\n",
       " ['We', 'lost'],\n",
       " ['We', 'lost'],\n",
       " ['We', 'lost'],\n",
       " ['We', 'lost'],\n",
       " ['We', 'lost'],\n",
       " ['We', 'lost'],\n",
       " ['We', 'lost'],\n",
       " ['Welcome'],\n",
       " ['Who', 'won'],\n",
       " ['Who', 'won'],\n",
       " ['You', 'run'],\n",
       " ['Am', 'I', 'fat'],\n",
       " ['Am', 'I', 'fat'],\n",
       " ['Ask', 'them'],\n",
       " ['Ask', 'them'],\n",
       " ['Back', 'off'],\n",
       " ['Back', 'off'],\n",
       " ['Back', 'off'],\n",
       " ['Back', 'off'],\n",
       " ['Back', 'off'],\n",
       " ['Back', 'off'],\n",
       " ['Be', 'a', 'man'],\n",
       " ['Be', 'a', 'man'],\n",
       " ['Be', 'still'],\n",
       " ['Be', 'still'],\n",
       " ['Be', 'still'],\n",
       " ['Beats', 'me'],\n",
       " ['Beats', 'me'],\n",
       " ['Call', 'Tom'],\n",
       " ['Call', 'Tom'],\n",
       " ['Cheer', 'up'],\n",
       " ['Cool', 'off'],\n",
       " ['Cuff', 'him'],\n",
       " ['Drive', 'on'],\n",
       " ['Drive', 'on'],\n",
       " ['Drive', 'on'],\n",
       " ['Drive', 'on'],\n",
       " ['Find', 'Tom'],\n",
       " ['Find', 'Tom'],\n",
       " ['Fix', 'this'],\n",
       " ['Fix', 'this'],\n",
       " ['Get', 'down'],\n",
       " ['Get', 'down'],\n",
       " ['Get', 'down'],\n",
       " ['Get', 'down'],\n",
       " ['Get', 'down'],\n",
       " ['Get', 'lost'],\n",
       " ['Get', 'lost'],\n",
       " ['Get', 'lost'],\n",
       " ['Get', 'real'],\n",
       " ['Go', 'ahead'],\n",
       " ['Go', 'ahead'],\n",
       " ['Go', 'ahead'],\n",
       " ['Go', 'ahead'],\n",
       " ['Go', 'ahead'],\n",
       " ['Go', 'ahead'],\n",
       " ['Go', 'ahead'],\n",
       " ['Good', 'job'],\n",
       " ['Good', 'job'],\n",
       " ['Good', 'job'],\n",
       " ['Grab', 'him'],\n",
       " ['Grab', 'him'],\n",
       " ['Have', 'fun'],\n",
       " ['Have', 'fun'],\n",
       " ['He', 'tries'],\n",
       " ['Hes', 'wet'],\n",
       " ['Help', 'Tom'],\n",
       " ['Help', 'Tom'],\n",
       " ['Hi', 'guys'],\n",
       " ['How', 'cute'],\n",
       " ['How', 'deep'],\n",
       " ['How', 'nice'],\n",
       " ['How', 'nice'],\n",
       " ['How', 'nice'],\n",
       " ['How', 'nice'],\n",
       " ['Hurry', 'up'],\n",
       " ['Hurry', 'up'],\n",
       " ['Hurry', 'up'],\n",
       " ['Hurry', 'up'],\n",
       " ['I', 'did', 'OK'],\n",
       " ['I', 'did', 'OK'],\n",
       " ['I', 'did', 'it'],\n",
       " ['I', 'did', 'it'],\n",
       " ['I', 'failed'],\n",
       " ['I', 'forgot'],\n",
       " ['I', 'get', 'it'],\n",
       " ['I', 'got', 'it'],\n",
       " ['I', 'got', 'it'],\n",
       " ['I', 'helped'],\n",
       " ['I', 'jumped'],\n",
       " ['I', 'looked'],\n",
       " ['I', 'moaned'],\n",
       " ['I', 'nodded'],\n",
       " ['I', 'obeyed'],\n",
       " ['I', 'phoned'],\n",
       " ['I', 'phoned'],\n",
       " ['I', 'refuse'],\n",
       " ['I', 'refuse'],\n",
       " ['I', 'rested'],\n",
       " ['I', 'rested'],\n",
       " ['I', 'saw', 'it'],\n",
       " ['I', 'saw', 'it'],\n",
       " ['I', 'sighed'],\n",
       " ['I', 'stayed'],\n",
       " ['I', 'stayed'],\n",
       " ['I', 'talked'],\n",
       " ['I', 'use', 'it'],\n",
       " ['I', 'use', 'it'],\n",
       " ['I', 'use', 'it'],\n",
       " ['Ill', 'pay'],\n",
       " ['Ill', 'try'],\n",
       " ['Ill', 'try'],\n",
       " ['Im', 'back'],\n",
       " ['Im', 'back'],\n",
       " ['Im', 'bald'],\n",
       " ['Im', 'busy'],\n",
       " ['Im', 'busy'],\n",
       " ['Im', 'calm'],\n",
       " ['Im', 'cold'],\n",
       " ['Im', 'cool'],\n",
       " ['Im', 'cool'],\n",
       " ['Im', 'deaf'],\n",
       " ['Im', 'deaf'],\n",
       " ['Im', 'done'],\n",
       " ['Im', 'fair'],\n",
       " ['Im', 'fair'],\n",
       " ['Im', 'fair'],\n",
       " ['Im', 'fast'],\n",
       " ['Im', 'fine'],\n",
       " ['Im', 'fine'],\n",
       " ['Im', 'fine'],\n",
       " ['Im', 'free'],\n",
       " ['Im', 'free'],\n",
       " ['Im', 'free'],\n",
       " ['Im', 'full'],\n",
       " ['Im', 'full'],\n",
       " ['Im', 'game'],\n",
       " ['Im', 'game'],\n",
       " ['Im', 'glad'],\n",
       " ['Im', 'home'],\n",
       " ['Im', 'late'],\n",
       " ['Im', 'lazy'],\n",
       " ['Im', 'lazy'],\n",
       " ['Im', 'lazy'],\n",
       " ['Im', 'lazy'],\n",
       " ['Im', 'okay'],\n",
       " ['Im', 'okay'],\n",
       " ['Im', 'rich'],\n",
       " ['Im', 'safe'],\n",
       " ['Im', 'sick'],\n",
       " ['Im', 'sure'],\n",
       " ['Im', 'sure'],\n",
       " ['Im', 'sure'],\n",
       " ['Im', 'sure'],\n",
       " ['Im', 'tall'],\n",
       " ['Im', 'thin'],\n",
       " ['Im', 'tidy'],\n",
       " ['Im', 'tidy'],\n",
       " ['Im', 'ugly'],\n",
       " ['Im', 'ugly'],\n",
       " ['Im', 'weak'],\n",
       " ['Im', 'well'],\n",
       " ['Im', 'well'],\n",
       " ['Ive', 'won'],\n",
       " ['Ive', 'won'],\n",
       " ['It', 'helps'],\n",
       " ['It', 'hurts'],\n",
       " ['It', 'works'],\n",
       " ['It', 'works'],\n",
       " ['Its', 'Tom'],\n",
       " ['Its', 'fun'],\n",
       " ['Its', 'fun'],\n",
       " ['Its', 'his'],\n",
       " ['Its', 'his'],\n",
       " ['Its', 'new'],\n",
       " ['Its', 'new'],\n",
       " ['Its', 'odd'],\n",
       " ['Its', 'red'],\n",
       " ['Its', 'sad'],\n",
       " ['Keep', 'out'],\n",
       " ['Keep', 'out'],\n",
       " ['Kiss', 'Tom'],\n",
       " ['Leave', 'it'],\n",
       " ['Leave', 'it'],\n",
       " ['Leave', 'me'],\n",
       " ['Leave', 'us'],\n",
       " ['Leave', 'us'],\n",
       " ['Lets', 'go'],\n",
       " ['Lets', 'go'],\n",
       " ['Look', 'out'],\n",
       " ['Look', 'out'],\n",
       " ['Marry', 'me'],\n",
       " ['Marry', 'me'],\n",
       " ['May', 'I', 'go'],\n",
       " ['May', 'I', 'go'],\n",
       " ['May', 'I', 'go'],\n",
       " ['Save', 'Tom'],\n",
       " ['Save', 'Tom'],\n",
       " ['Say', 'what'],\n",
       " ['She', 'came'],\n",
       " ['She', 'died'],\n",
       " ['She', 'runs'],\n",
       " ['Sit', 'down'],\n",
       " ['Sit', 'down'],\n",
       " ['Sit', 'down'],\n",
       " ['Sit', 'here'],\n",
       " ['Sit', 'here'],\n",
       " ['Speak', 'up'],\n",
       " ['Speak', 'up'],\n",
       " ['Stand', 'up'],\n",
       " ['Stop', 'Tom'],\n",
       " ['Stop', 'Tom'],\n",
       " ['Taste', 'it'],\n",
       " ['Taste', 'it'],\n",
       " ['Taste', 'it'],\n",
       " ['Taste', 'it'],\n",
       " ['Tell', 'Tom'],\n",
       " ['Tell', 'Tom'],\n",
       " ['Terrific'],\n",
       " ['Terrific'],\n",
       " ['Terrific'],\n",
       " ['They', 'won'],\n",
       " ['They', 'won'],\n",
       " ['They', 'won'],\n",
       " ['They', 'won'],\n",
       " ['Tom', 'came'],\n",
       " ['Tom', 'died'],\n",
       " ['Tom', 'knew'],\n",
       " ['Tom', 'left'],\n",
       " ['Tom', 'left'],\n",
       " ['Tom', 'lied'],\n",
       " ['Tom', 'lies'],\n",
       " ['Tom', 'lost'],\n",
       " ['Tom', 'paid'],\n",
       " ['Too', 'late'],\n",
       " ['Trust', 'me'],\n",
       " ['Trust', 'me'],\n",
       " ['Try', 'hard'],\n",
       " ['Try', 'some'],\n",
       " ['Try', 'some'],\n",
       " ['Try', 'this'],\n",
       " ['Try', 'this'],\n",
       " ['Use', 'this'],\n",
       " ['Use', 'this'],\n",
       " ['Use', 'this'],\n",
       " ['Use', 'this'],\n",
       " ['Warn', 'Tom'],\n",
       " ['Warn', 'Tom'],\n",
       " ['Watch', 'me'],\n",
       " ['Watch', 'me'],\n",
       " ['Watch', 'us'],\n",
       " ['Watch', 'us'],\n",
       " ['We', 'agree'],\n",
       " ['Well', 'go'],\n",
       " ['Were', 'OK'],\n",
       " ['What', 'for'],\n",
       " ['What', 'for'],\n",
       " ['What', 'fun'],\n",
       " ['What', 'fun'],\n",
       " ['Who', 'came'],\n",
       " ['Who', 'died'],\n",
       " ['Who', 'fell'],\n",
       " ['Who', 'quit'],\n",
       " ['Whos', 'he'],\n",
       " ['Write', 'me'],\n",
       " ['Write', 'me'],\n",
       " ['You', 'lost'],\n",
       " ['You', 'lost'],\n",
       " ['After', 'you'],\n",
       " ['Aim', 'Fire'],\n",
       " ['Am', 'I', 'late'],\n",
       " ['Answer', 'me'],\n",
       " ['Be', 'seated'],\n",
       " ['Be', 'seated'],\n",
       " ['Birds', 'fly'],\n",
       " ['Bless', 'you'],\n",
       " ['Call', 'home'],\n",
       " ['Calm', 'down'],\n",
       " ['Calm', 'down'],\n",
       " ['Can', 'we', 'go'],\n",
       " ['Can', 'we', 'go'],\n",
       " ['Can', 'we', 'go'],\n",
       " ['Catch', 'Tom'],\n",
       " ['Catch', 'Tom'],\n",
       " ['Catch', 'him'],\n",
       " ['Chill', 'out'],\n",
       " ['Come', 'back'],\n",
       " ['Come', 'back'],\n",
       " ['Come', 'here'],\n",
       " ['Come', 'here'],\n",
       " ['Come', 'over'],\n",
       " ['Come', 'over'],\n",
       " ['Come', 'over'],\n",
       " ['Come', 'over'],\n",
       " ['Come', 'over'],\n",
       " ['Come', 'over'],\n",
       " ['Come', 'over'],\n",
       " ['Come', 'soon'],\n",
       " ['Come', 'soon'],\n",
       " ['Cool', 'down'],\n",
       " ['Did', 'I', 'win'],\n",
       " ['Did', 'I', 'win'],\n",
       " ['Did', 'I', 'win'],\n",
       " ['Do', 'it', 'now'],\n",
       " ['Dogs', 'bark'],\n",
       " ['Dogs', 'bark'],\n",
       " ['Dont', 'ask'],\n",
       " ['Dont', 'cry'],\n",
       " ['Dont', 'die'],\n",
       " ['Dont', 'die'],\n",
       " ['Dont', 'lie'],\n",
       " ['Dont', 'run'],\n",
       " ['Dont', 'run'],\n",
       " ['Excuse', 'me'],\n",
       " ['Excuse', 'me'],\n",
       " ['Excuse', 'me'],\n",
       " ['Excuse', 'me'],\n",
       " ['Excuse', 'me'],\n",
       " ['Excuse', 'me'],\n",
       " ['Excuse', 'me'],\n",
       " ['Fantastic'],\n",
       " ['Feel', 'this'],\n",
       " ['Feel', 'this'],\n",
       " ['Feel', 'this'],\n",
       " ['Feel', 'this'],\n",
       " ['Follow', 'me'],\n",
       " ['Follow', 'us'],\n",
       " ['Follow', 'us'],\n",
       " ['Forget', 'it'],\n",
       " ['Forget', 'it'],\n",
       " ['Forget', 'it'],\n",
       " ['Forget', 'it'],\n",
       " ['Forget', 'it'],\n",
       " ['Forget', 'it'],\n",
       " ['Forget', 'it'],\n",
       " ['Get', 'a', 'job'],\n",
       " ['Get', 'a', 'job'],\n",
       " ['Get', 'a', 'job'],\n",
       " ['Get', 'a', 'job'],\n",
       " ['Get', 'ready'],\n",
       " ['Get', 'ready'],\n",
       " ['Go', 'get', 'it'],\n",
       " ['Go', 'get', 'it'],\n",
       " ['Go', 'inside'],\n",
       " ['Go', 'to', 'bed'],\n",
       " ['Go', 'to', 'bed'],\n",
       " ['Good', 'luck'],\n",
       " ['Good', 'luck'],\n",
       " ['Grab', 'that'],\n",
       " ['Grab', 'that'],\n",
       " ['Grab', 'that'],\n",
       " ['Grab', 'that'],\n",
       " ['Grab', 'this'],\n",
       " ['Grab', 'this'],\n",
       " ['Hands', 'off'],\n",
       " ['He', 'is', 'ill'],\n",
       " ['He', 'is', 'old'],\n",
       " ['Hes', 'a', 'DJ'],\n",
       " ['Hes', 'good'],\n",
       " ['Hes', 'lazy'],\n",
       " ['Hes', 'mine'],\n",
       " ['Hes', 'rich'],\n",
       " ['Hes', 'sexy'],\n",
       " ['Here', 'I', 'am'],\n",
       " ['Heres', '5'],\n",
       " ['Hold', 'fire'],\n",
       " ['Hold', 'fire'],\n",
       " ['Hold', 'this'],\n",
       " ['Hold', 'this'],\n",
       " ['Hold', 'this'],\n",
       " ['Hold', 'this'],\n",
       " ['How', 'awful'],\n",
       " ['How', 'weird'],\n",
       " ['Hows', 'Tom'],\n",
       " ['Hows', 'Tom'],\n",
       " ['I', 'am', 'cold'],\n",
       " ['I', 'am', 'good'],\n",
       " ['I', 'am', 'okay'],\n",
       " ['I', 'am', 'sick'],\n",
       " ['I', 'am', 'sure'],\n",
       " ['I', 'am', 'sure'],\n",
       " ['I', 'beg', 'you'],\n",
       " ['I', 'beg', 'you'],\n",
       " ['I', 'beg', 'you'],\n",
       " ['I', 'beg', 'you'],\n",
       " ['I', 'can', 'run'],\n",
       " ['I', 'can', 'ski'],\n",
       " ['I', 'cringed'],\n",
       " ['I', 'cringed'],\n",
       " ['I', 'cringed'],\n",
       " ['I', 'exhaled'],\n",
       " ['I', 'gave', 'up'],\n",
       " ['I', 'give', 'in'],\n",
       " ['I', 'give', 'up'],\n",
       " ['I', 'got', 'hot'],\n",
       " ['I', 'got', 'hot'],\n",
       " ['I', 'had', 'fun'],\n",
       " ['I', 'had', 'fun'],\n",
       " ['I', 'had', 'fun'],\n",
       " ['I', 'had', 'fun'],\n",
       " ['I', 'hate', 'it'],\n",
       " ['I', 'have', 'it'],\n",
       " ['I', 'hit', 'Tom'],\n",
       " ['I', 'hope', 'so'],\n",
       " ['I', 'hurried'],\n",
       " ['I', 'hurried'],\n",
       " ['I', 'inhaled'],\n",
       " ['I', 'knew', 'it'],\n",
       " ['I', 'like', 'it'],\n",
       " ['I', 'lost', 'it'],\n",
       " ['I', 'love', 'it'],\n",
       " ['I', 'love', 'it'],\n",
       " ['I', 'mean', 'it'],\n",
       " ['I', 'mean', 'it'],\n",
       " ['I', 'must', 'go'],\n",
       " ['I', 'must', 'go'],\n",
       " ['I', 'must', 'go'],\n",
       " ['I', 'must', 'go'],\n",
       " ['I', 'must', 'go'],\n",
       " ['I', 'must', 'go'],\n",
       " ['I', 'must', 'go'],\n",
       " ['I', 'must', 'go'],\n",
       " ['I', 'need', 'it'],\n",
       " ['I', 'need', 'it'],\n",
       " ['I', 'noticed'],\n",
       " ['I', 'prepaid'],\n",
       " ['I', 'promise'],\n",
       " ['I', 'relaxed'],\n",
       " ['I', 'relaxed'],\n",
       " ['I', 'retired'],\n",
       " ['I', 'said', 'no'],\n",
       " ['I', 'said', 'so'],\n",
       " ['I', 'saw', 'him'],\n",
       " ['I', 'saw', 'him'],\n",
       " ['I', 'saw', 'him'],\n",
       " ['I', 'saw', 'one'],\n",
       " ['I', 'saw', 'one'],\n",
       " ['I', 'saw', 'you'],\n",
       " ['I', 'saw', 'you'],\n",
       " ['I', 'saw', 'you'],\n",
       " ['I', 'saw', 'you'],\n",
       " ['I', 'saw', 'you'],\n",
       " ['I', 'saw', 'you'],\n",
       " ['I', 'saw', 'you'],\n",
       " ['I', 'saw', 'you'],\n",
       " ['I', 'see', 'Tom'],\n",
       " ['I', 'shouted'],\n",
       " ['I', 'tripped'],\n",
       " ['I', 'tripped'],\n",
       " ['I', 'want', 'it'],\n",
       " ['I', 'was', 'new'],\n",
       " ['I', 'was', 'new'],\n",
       " ['I', 'will', 'go'],\n",
       " ['I', 'woke', 'up'],\n",
       " ['I', 'woke', 'up'],\n",
       " ['Id', 'agree'],\n",
       " ['Id', 'leave'],\n",
       " ['Ill', 'call'],\n",
       " ['Ill', 'cook'],\n",
       " ['Ill', 'help'],\n",
       " ['Ill', 'live'],\n",
       " ['Ill', 'obey'],\n",
       " ['Ill', 'pack'],\n",
       " ['Ill', 'pack'],\n",
       " ['Ill', 'pack'],\n",
       " ['Ill', 'pass'],\n",
       " ['Ill', 'quit'],\n",
       " ['Ill', 'sing'],\n",
       " ['Ill', 'stop'],\n",
       " ['Ill', 'swim'],\n",
       " ['Ill', 'talk'],\n",
       " ['Ill', 'talk'],\n",
       " ['Ill', 'wait'],\n",
       " ['Ill', 'walk'],\n",
       " ['Ill', 'work'],\n",
       " ['Ill', 'work'],\n",
       " ['Im', 'a', 'cop'],\n",
       " ['Im', 'a', 'man'],\n",
       " ['Im', 'alive'],\n",
       " ['Im', 'alive'],\n",
       " ['Im', 'alive'],\n",
       " ['Im', 'alone'],\n",
       " ['Im', 'alone'],\n",
       " ['Im', 'angry'],\n",
       " ['Im', 'angry'],\n",
       " ['Im', 'armed'],\n",
       " ['Im', 'armed'],\n",
       " ['Im', 'awake'],\n",
       " ['Im', 'blind'],\n",
       " ['Im', 'broke'],\n",
       " ['Im', 'clean'],\n",
       " ['Im', 'clean'],\n",
       " ['Im', 'crazy'],\n",
       " ['Im', 'crazy'],\n",
       " ['Im', 'cured'],\n",
       " ['Im', 'cured'],\n",
       " ['Im', 'dizzy'],\n",
       " ['Im', 'drunk'],\n",
       " ['Im', 'drunk'],\n",
       " ['Im', 'drunk'],\n",
       " ['Im', 'dying'],\n",
       " ['Im', 'early'],\n",
       " ['Im', 'first'],\n",
       " ['Im', 'fussy'],\n",
       " ['Im', 'fussy'],\n",
       " ['Im', 'fussy'],\n",
       " ['Im', 'going'],\n",
       " ['Im', 'going'],\n",
       " ['Im', 'going'],\n",
       " ['Im', 'going'],\n",
       " ['Im', 'loyal'],\n",
       " ['Im', 'loyal'],\n",
       " ['Im', 'lucky'],\n",
       " ['Im', 'lucky'],\n",
       " ['Im', 'lucky'],\n",
       " ['Im', 'lucky'],\n",
       " ['Im', 'lucky'],\n",
       " ['Im', 'lying'],\n",
       " ['Im', 'naked'],\n",
       " ['Im', 'naked'],\n",
       " ['Im', 'naked'],\n",
       " ['Im', 'naked'],\n",
       " ['Im', 'naked'],\n",
       " ['Im', 'quiet'],\n",
       " ['Im', 'ready'],\n",
       " ['Im', 'ready'],\n",
       " ['Im', 'ready'],\n",
       " ['Im', 'right'],\n",
       " ['Im', 'sober'],\n",
       " ['Im', 'sorry'],\n",
       " ['Im', 'sorry'],\n",
       " ['Im', 'sorry'],\n",
       " ['Im', 'sorry'],\n",
       " ['Im', 'sorry'],\n",
       " ['Im', 'sorry'],\n",
       " ['Im', 'stuck'],\n",
       " ['Im', 'timid'],\n",
       " ['Im', 'tired'],\n",
       " ['Im', 'tough'],\n",
       " ['Im', 'tough'],\n",
       " ['Im', 'tough'],\n",
       " ['Im', 'tough'],\n",
       " ['Im', 'yours'],\n",
       " ['Im', 'yours'],\n",
       " ['Ive', 'lost'],\n",
       " ['Is', 'Tom', 'OK'],\n",
       " ['Is', 'Tom', 'OK'],\n",
       " ['Is', 'it', 'bad'],\n",
       " ['Is', 'it', 'far'],\n",
       " ['Is', 'it', 'far'],\n",
       " ['Is', 'it', 'hot'],\n",
       " ['Is', 'it', 'you'],\n",
       " ['Is', 'it', 'you'],\n",
       " ['Is', 'it', 'you'],\n",
       " ['It', 'failed'],\n",
       " ['It', 'snowed'],\n",
       " ['It', 'stinks'],\n",
       " ['It', 'stinks'],\n",
       " ['It', 'was', 'OK'],\n",
       " ['It', 'was', 'OK'],\n",
       " ['It', 'was', 'OK'],\n",
       " ['It', 'worked'],\n",
       " ['It', 'worked'],\n",
       " ['Its', '330'],\n",
       " ['Its', '830'],\n",
       " ['Its', '830'],\n",
       " ['Its', 'a', 'TV'],\n",
       " ['Its', 'cold'],\n",
       " ['Its', 'cold'],\n",
       " ['Its', 'dark'],\n",
       " ['Its', 'dead'],\n",
       " ['Its', 'dead'],\n",
       " ['Its', 'dead'],\n",
       " ['Its', 'done'],\n",
       " ['Its', 'easy'],\n",
       " ['Its', 'food'],\n",
       " ['Its', 'free'],\n",
       " ['Its', 'here'],\n",
       " ['Its', 'here'],\n",
       " ['Its', 'hers'],\n",
       " ['Its', 'hers'],\n",
       " ['Its', 'late'],\n",
       " ['Its', 'lost'],\n",
       " ['Its', 'mine'],\n",
       " ['Its', 'mine'],\n",
       " ['Its', 'mine'],\n",
       " ['Its', 'mine'],\n",
       " ['Its', 'open'],\n",
       " ['Its', 'ours'],\n",
       " ['Its', 'ours'],\n",
       " ['Its', 'ours'],\n",
       " ['Its', 'sand'],\n",
       " ['Its', 'time'],\n",
       " ['Its', 'time'],\n",
       " ['Its', 'true'],\n",
       " ['Its', 'work'],\n",
       " ['Keep', 'calm'],\n",
       " ['Keep', 'that'],\n",
       " ['Keep', 'that'],\n",
       " ['Keep', 'this'],\n",
       " ['Keep', 'this'],\n",
       " ['Let', 'it', 'be'],\n",
       " ['Let', 'it', 'be'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'go'],\n",
       " ['Let', 'me', 'in'],\n",
       " ['Let', 'me', 'in'],\n",
       " ['Lets', 'ask'],\n",
       " ['Lets', 'eat'],\n",
       " ['Lets', 'see'],\n",
       " ['Lie', 'still'],\n",
       " ['Lie', 'still'],\n",
       " ['Lie', 'still'],\n",
       " ['Lie', 'still'],\n",
       " ['Lie', 'still'],\n",
       " ['Lie', 'still'],\n",
       " ['Look', 'away'],\n",
       " ['Look', 'away'],\n",
       " ['Look', 'back'],\n",
       " ['Look', 'back'],\n",
       " ['Look', 'here'],\n",
       " ['Look', 'here'],\n",
       " ['Loosen', 'up'],\n",
       " ['Loosen', 'up'],\n",
       " ['Loosen', 'up'],\n",
       " ['Loosen', 'up'],\n",
       " ['Loosen', 'up'],\n",
       " ['Move', 'over'],\n",
       " ['Move', 'over'],\n",
       " ['Move', 'over'],\n",
       " ['Nice', 'shot'],\n",
       " ['Of', 'course'],\n",
       " ['Of', 'course'],\n",
       " ['Of', 'course'],\n",
       " ['Of', 'course'],\n",
       " ['Of', 'course'],\n",
       " ['Oh', 'please'],\n",
       " ['Oh', 'please'],\n",
       " ['Pardon', 'me'],\n",
       " ['Pardon', 'me'],\n",
       " ['Pardon', 'me'],\n",
       " ['Read', 'this'],\n",
       " ['Say', 'hello'],\n",
       " ['See', 'above'],\n",
       " ['See', 'below'],\n",
       " ['See', 'below'],\n",
       " ['Seize', 'him'],\n",
       " ['Seize', 'him'],\n",
       " ['Seriously'],\n",
       " ['Seriously'],\n",
       " ['Seriously'],\n",
       " ['She', 'cried'],\n",
       " ['She', 'cried'],\n",
       " ['She', 'tried'],\n",
       " ['She', 'walks'],\n",
       " ['Shes', 'hot'],\n",
       " ['Shes', 'hot'],\n",
       " ['Sign', 'here'],\n",
       " ['Sign', 'here'],\n",
       " ['Sign', 'this'],\n",
       " ['Sign', 'this'],\n",
       " ['Slow', 'down'],\n",
       " ['Slow', 'down'],\n",
       " ['Stay', 'away'],\n",
       " ['Stay', 'away'],\n",
       " ['Stay', 'back'],\n",
       " ['Stay', 'back'],\n",
       " ['Stay', 'calm'],\n",
       " ['Stay', 'calm'],\n",
       " ['Stay', 'calm'],\n",
       " ['Stay', 'calm'],\n",
       " ['Stay', 'calm'],\n",
       " ['Stay', 'down'],\n",
       " ['Stay', 'down'],\n",
       " ['Stay', 'down'],\n",
       " ['Stay', 'down'],\n",
       " ['Stay', 'here'],\n",
       " ['Stay', 'here'],\n",
       " ['Stay', 'here'],\n",
       " ['Stay', 'thin'],\n",
       " ['Step', 'back'],\n",
       " ['Step', 'back'],\n",
       " ['Stop', 'that'],\n",
       " ['Stop', 'that'],\n",
       " ['Stop', 'that'],\n",
       " ['Stop', 'that'],\n",
       " ['Stop', 'them'],\n",
       " ['Take', 'care'],\n",
       " ['Take', 'care'],\n",
       " ['Take', 'care'],\n",
       " ['Take', 'care'],\n",
       " ['Take', 'mine'],\n",
       " ['Take', 'mine'],\n",
       " ['Take', 'mine'],\n",
       " ['Take', 'mine'],\n",
       " ['Take', 'mine'],\n",
       " ['Take', 'mine'],\n",
       " ['Take', 'mine'],\n",
       " ['Take', 'mine'],\n",
       " ['Take', 'this'],\n",
       " ['Take', 'this'],\n",
       " ['Thank', 'you'],\n",
       " ['Thank', 'you'],\n",
       " ['Thats', 'OK'],\n",
       " ['Thats', 'OK'],\n",
       " ['Thats', 'OK'],\n",
       " ['Thats', 'OK'],\n",
       " ['Thats', 'it'],\n",
       " ['Then', 'what'],\n",
       " ['They', 'fell'],\n",
       " ['They', 'fell'],\n",
       " ['They', 'left'],\n",
       " ['They', 'left'],\n",
       " ['They', 'lied'],\n",
       " ['They', 'lied'],\n",
       " ['They', 'lost'],\n",
       " ['They', 'lost'],\n",
       " ['They', 'swam'],\n",
       " ['They', 'swam'],\n",
       " ['They', 'swam'],\n",
       " ['They', 'swam'],\n",
       " ['Times', 'up'],\n",
       " ['Tom', 'cooks'],\n",
       " ['Tom', 'cried'],\n",
       " ['Tom', 'is', 'OK'],\n",
       " ['Tom', 'knits'],\n",
       " ['Tom', 'knows'],\n",
       " ['Tom', 'rocks'],\n",
       " ['Tom', 'spoke'],\n",
       " ['Tom', 'waved'],\n",
       " ['Tom', 'works'],\n",
       " ['Toms', 'fat'],\n",
       " ['Toms', 'mad'],\n",
       " ['Toms', 'mad'],\n",
       " ['Toms', 'sad'],\n",
       " ['Trust', 'Tom'],\n",
       " ['Trust', 'Tom'],\n",
       " ['Try', 'again'],\n",
       " ['Try', 'again'],\n",
       " ['Try', 'again'],\n",
       " ['Try', 'it', 'on'],\n",
       " ['Turn', 'left'],\n",
       " ['Wait', 'here'],\n",
       " ['Wait', 'here'],\n",
       " ['Wait', 'here'],\n",
       " ['Wait', 'here'],\n",
       " ['Watch', 'out'],\n",
       " ['Watch', 'out'],\n",
       " ['Watch', 'out'],\n",
       " ['We', 'agreed'],\n",
       " ['We', 'did', 'it'],\n",
       " ['We', 'did', 'it'],\n",
       " ['We', 'did', 'it'],\n",
       " ['We', 'forgot'],\n",
       " ['We', 'saw', 'it'],\n",
       " ['We', 'saw', 'it'],\n",
       " ['We', 'smiled'],\n",
       " ['We', 'talked'],\n",
       " ['We', 'talked'],\n",
       " ['We', 'talked'],\n",
       " ['We', 'talked'],\n",
       " ['We', 'talked'],\n",
       " ['We', 'waited'],\n",
       " ['We', 'waited'],\n",
       " ['We', 'walked'],\n",
       " ['We', 'walked'],\n",
       " ['Well', 'try'],\n",
       " ['Well', 'try'],\n",
       " ['Well', 'win'],\n",
       " ['Well', 'win'],\n",
       " ['Were', 'hot'],\n",
       " ['Were', 'sad'],\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_Eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b794a86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175621"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_Fre = [nltk.sent_tokenize(word) for word in No_Punc_Fre]\n",
    "len(tokenized_Fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34d8dba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Bildad\n",
      "[nltk_data]     Otieno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68ab5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying if that we have English and French Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0aebf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_Eng = stopwords.words('english') #179 of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5e76682",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_Fre = stopwords.words('french') #157 of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6163df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Stop_Eng = []\n",
    "for word in tokenized_Eng:\n",
    "    if word not in stop_Eng:\n",
    "        No_Stop_Eng.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bfd1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175621"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(No_Stop_Eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57d5ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Stop_Fre = []\n",
    "for word in tokenized_Fre:\n",
    "    if word not in stop_Fre:\n",
    "        No_Stop_Fre.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "558ced41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175621"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(No_Stop_Fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5df7dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_Fre = []\n",
    "for words in No_Stop_Fre:\n",
    "    for word in words:\n",
    "        lower_Fre.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce0ad397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175621"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lower_Fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fba9b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lower_Fre,columns= [\"French Words\"])\n",
    "df2 = pd.DataFrame(lower_Fre,columns= [\"French Words\"])\n",
    "#df.to_parquet(\"C:\\\\Users\\\\Bildad Otieno\\\\Documents\\\\Billy_Repo\\\\Translation_Mod\\\\French.parquet\")\n",
    "df2.to_csv(\"C:\\\\Users\\\\Bildad Otieno\\\\Documents\\\\Billy_Repo\\\\Translation_Mod\\\\French.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "082cb400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 188 ms\n",
      "Wall time: 232 ms\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 161 ms\n"
     ]
    }
   ],
   "source": [
    "%time French = pd.read_parquet(\"French.parquet\")\n",
    "%time French2 = pd.read_csv(\"French.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efc6b482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120297"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "French = pd.read_parquet(\"French.parquet\")\n",
    "len(French)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35790dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "French2 = pd.read_csv(\"French.csv\", usecols=lambda col: col != 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85ea6fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>French Words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-parquet, 1 graph layer</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              French Words\n",
       "npartitions=1             \n",
       "                    object\n",
       "                       ...\n",
       "Dask Name: read-parquet, 1 graph layer"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "FrenchPar = dd.read_parquet(\"French.parquet\")\n",
    "FrenchPar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05a84bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FrenchPar = FrenchPar.repartition(npartitions=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f59b26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FrenchPar.to_parquet(r\"C:\\Users\\Bildad Otieno\\Documents\\Billy_Repo\\Translation_Mod\\French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcb6b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_Eng = []\n",
    "for words in No_Stop_Eng:\n",
    "    for word in words:\n",
    "        lower_Eng.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e4d34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(lower_Eng,columns= [\"English Words\"])\n",
    "df3.to_parquet(\"C:\\\\Users\\\\Bildad Otieno\\\\Documents\\\\Billy_Repo\\\\Translation_Mod\\\\English.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4424c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120297"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EnglishPar = dd.read_parquet(r\"C:\\Users\\Bildad Otieno\\Documents\\Billy_Repo\\Translation_Mod\\English.parquet\")\n",
    "len(EnglishPar)\n",
    "len(FrenchPar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de1fdc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EnglishPar.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2707308",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnglishPar = EnglishPar.repartition(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d7f310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English Words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=20</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: repartition, 2 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               English Words\n",
       "npartitions=20              \n",
       "                      object\n",
       "                         ...\n",
       "...                      ...\n",
       "                         ...\n",
       "                         ...\n",
       "Dask Name: repartition, 2 graph layers"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EnglishPar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9cf29fa",
   "metadata": {},
   "source": [
    "I will opt for lemmatization and not stemming as I did before:\n",
    "\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    print(\" {0:25}  {1:25} \".format(\"--Word(s)--\",\"--Stem--\"))\n",
    "    for word in lower_Eng:\n",
    "        print(\"   {0:25}  {1:25} \".format(word,ps.stem(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01724e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all') - Every Package is Up-to-date for my Ellie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7510e726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Bildad\n",
      "[nltk_data]     Otieno/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22ff8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ae39705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Bildad\n",
      "[nltk_data]     Otieno/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "'''print(\" {0:25}  {1:25} \".format(\"--Word(s)--\",\"--Lemma--\"))\n",
    "for word in lower_Fre:\n",
    "    print(\"   {0:25}  {1:25} \".format(word, wnl.lemmatize(word, pos='v')))'''\n",
    "    \n",
    "lemm_Eng = [wnl.lemmatize(word, pos='v') for word in lower_Eng]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04edf951",
   "metadata": {},
   "source": [
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e682391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "Part0 = dd.read_parquet(r\"C:\\Users\\Bildad Otieno\\Documents\\Billy_Repo\\Translation_Mod\\French\\part.0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7cd9da93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "FreWord = []\n",
    "for word in FrenchPar.to_delayed():\n",
    "    FreWord.append(word.compute())\n",
    "FreWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_col = []\n",
    "for text in FreWord:\n",
    "    doc = nlp(str(text))\n",
    "    col = []\n",
    "    for word in doc:\n",
    "        col.append(word.lemma_)\n",
    "    new_word = \" \".join(col)\n",
    "    new_col.append(new_word)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68012fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8f4ec60",
   "metadata": {},
   "source": [
    "## Splitting Dataset into 70:30 Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "791f51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eng_train, Eng_test, Fre_train, Fre_test = train_test_split(Eng, Fre, test_size= .33, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
